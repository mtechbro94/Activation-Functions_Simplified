# Activation Functions in Neural Networks

This repository contains a Google Colab notebook explaining **Activation Functions** in neural networks. The notebook covers both theoretical concepts and hands-on Python implementations. It is designed to help beginners and advanced learners understand the importance of activation functions in introducing non-linearity to neural networks.

---

## **Contents**
1. **Theory**
   - Role of activation functions
   - Properties and importance
   - Types of activation functions:
     - Linear
     - Sigmoid
     - Tanh
     - ReLU
     - Leaky ReLU
     - Softmax

2. **Visualization**
   - Plots to compare activation functions (Sigmoid, Tanh, ReLU, Leaky ReLU).

3. **Hands-On Examples**
   - Synthetic dataset generation.
   - Building a neural network using TensorFlow/Keras.
   - Comparing performance with different activation functions.

---

## **Requirements**
To run the notebook, you need the following:
- Python 3.7+
- Libraries: `numpy`, `matplotlib`, `tensorflow`, `sklearn`

Install them using:
```bash
pip install numpy matplotlib tensorflow scikit-learn
